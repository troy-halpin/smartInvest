{"nbformat_minor": 1, "cells": [{"source": "# This section will read the data from the Blackrock Aladdin API\n## The first step is to identify what stocks to read would have built out a full function where all that was needed to be passed in is the stock ticker name", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import csv\nfrom pyspark.sql.types import *\nfrom StringIO import StringIO\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1474704881191_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-troy.kigaoixnx5fetpyohrqjm3h3ke.bx.internal.cloudapp.net:8088/proxy/application_1474704881191_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.11:30060/node/containerlogs/container_1474704881191_0007_01_000001/spark\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "Creating HiveContext as 'sqlContext'\nSparkContext and HiveContext created. Executing user code ...\n"}], "metadata": {"collapsed": false}}, {"source": "### Make API call to Aladdin", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "from urllib2 import Request, urlopen, URLError\n\ndef aladdinCall(stockToken):\n    \n    request = Request(\"https://www.blackrock.com/tools/json/performance?identifiers=%s&outputFormat=json&useCache=true\" % stockToken)\n    try:\n        response = urlopen(request)\n        raw_stock_json = response.read()\n        return raw_stock_json\n    except URLError, e:\n        print 'Error:', e", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Parse the data  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 139, "cell_type": "code", "source": "import json\nfrom pyspark.sql import Row\nfrom collections import OrderedDict\n\ndef convert_to_row(d):\n    return Row(d)\n\nraw_data = aladdinCall('AAPL')\n\nstockJson = json.loads(raw_data)\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Convert Data to Data Frame", "cell_type": "markdown", "metadata": {}}, {"execution_count": 78, "cell_type": "code", "source": "#sinceStartDateAnnualized', u'oneMonth', u'level', u'sinceStartDate', u'asOfDate', u'drawdown', u'oneDay', u'@type'\n\ndataPoints = []\nfor obj in stockJson[\"resultMap\"][\"RETURNS\"][0]['returnsMap']:\n    temp = []\n    stop = False\n    for item in stockJson[\"resultMap\"][\"RETURNS\"][0]['returnsMap'][obj]:\n        if stockJson[\"resultMap\"][\"RETURNS\"][0]['returnsMap'][obj][item] != '':\n            temp.append(stockJson[\"resultMap\"][\"RETURNS\"][0]['returnsMap'][obj][item])\n        else:\n            stop = True\n    if len(temp) > 7 or stop:\n        #throw away data point, error in the processing\n        continue\n    dataPoints.append(temp)\n\nrdd = sc.parallelize(dataPoints)\n\n#print rdd\n\ndf = rdd.toDF(['sinceStartDateAnnualized', 'oneMonth','level','sinceStartDate','asOfDate',\n              'drawnDown', 'oneDay'])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Clean Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 186, "cell_type": "code", "source": "from pyspark.ml.feature import VectorAssembler\n\ndf = df.orderBy('sinceStartDate') # sort by date of month\n\ndf = df.drop('any')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# Start Machine Learning \n## Will train the linear regression model to identify level as the label, factors that we can use to determine that are the sinceStartDate, asOfDate, and sinceStartDateAnnualized\n### Cannot have any null values in the fields, and have use a map reduce function to create a list of Labeled points to store the data in", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 272, "cell_type": "code", "source": "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\nfrom pyspark.ml.regression import LinearRegression\n\nsqlContext.registerDataFrameAsTable(df, \"table1\")\n\ndf2 = sqlContext.sql(\"SELECT sinceStartDate AS f1, level as f2, asOfDate as f3, sinceStartDateAnnualized as f4 from table1\")\ndf2.collect()    \ndf2 = df2.orderBy('f1')\n\ndf3 = df2.filter(~ df2.f2.isNull() & ~ df2.f3.isNull() & ~ df2.f4.isNull())\n\ndef pointReduce(row):\n    temp = [float(x) for x in row]\n    return LabeledPoint(temp[1],[temp[0]] + temp[2:])\n\ndata = df3.map(pointReduce)\n", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "#### Here we actually train the linear regression model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 277, "cell_type": "code", "source": "model = LinearRegressionWithSGD.train(data, iterations=2, step=.00000001)\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 278, "cell_type": "code", "source": "temp = model.predict([20040402.0,0.0,182.666001102])\nprint temp", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-76782202546.7"}], "metadata": {"collapsed": false}}, {"source": "# livy Rest endpoint to my model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 235, "cell_type": "code", "source": "# Evaluate the model on training data\nvaluesAndPreds = data.map(lambda p: (p.label, model.predict(p.features)))\nMSE = valuesAndPreds \\\n    .map(lambda (v, p): (v - p)**2) \\\n    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\nprint(\"Mean Squared Error = \" + str(MSE))\n\n\n#Expose the algorithm here", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Mean Squared Error = 1.61657348901e+281"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}, "anaconda-cloud": {}}}